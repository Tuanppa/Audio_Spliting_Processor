# Architecture Documentation

Giáº£i thÃ­ch chi tiáº¿t kiáº¿n trÃºc vÃ  cÃ¡ch hoáº¡t Ä‘á»™ng cá»§a Audio Processor.

---

## ğŸ“ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         USER INPUT                          â”‚
â”‚                    (Audio Files + Config)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      ENTRY POINTS                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚   main.py    â”‚  â”‚  worker.py   â”‚  â”‚  example.py  â”‚     â”‚
â”‚  â”‚  (CLI Mode)  â”‚  â”‚ (Multi-node) â”‚  â”‚ (Python API) â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                  â”‚                  â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   PROCESSOR LAYER                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              AudioProcessor (processor.py)          â”‚   â”‚
â”‚  â”‚  â€¢ Orchestrates entire workflow                     â”‚   â”‚
â”‚  â”‚  â€¢ Manages single/batch processing                  â”‚   â”‚
â”‚  â”‚  â€¢ Handles errors and logging                       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                            â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   TRANSCRIBER     â”‚        â”‚    SEGMENTER     â”‚
    â”‚ (transcriber.py)  â”‚        â”‚  (segmenter.py)  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                            â”‚
              â–¼                            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Whisper + Stable-TS â”‚    â”‚   PyDub + SoundFile â”‚
    â”‚  (Speech Recognition)â”‚    â”‚  (Audio Processing) â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                            â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      OUTPUT LAYER                           â”‚
â”‚  â€¢ Individual audio/text files (segment_0001.wav/txt)      â”‚
â”‚  â€¢ Full transcript (text + JSON)                           â”‚
â”‚  â€¢ Manifest (metadata JSON)                                â”‚
â”‚  â€¢ Statistics (metadata.json)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ Core Components

### 1. **config.py** - Configuration Management

**Purpose:** Quáº£n lÃ½ táº¥t cáº£ cáº¥u hÃ¬nh cá»§a há»‡ thá»‘ng

**Key Classes:**
- `WhisperConfig`: Cáº¥u hÃ¬nh Whisper model (model_size, device, language)
- `AudioConfig`: Cáº¥u hÃ¬nh audio processing (sample_rate, format, segment duration)
- `ProcessConfig`: Cáº¥u hÃ¬nh xá»­ lÃ½ (output format, batch size, naming)
- `PathConfig`: Quáº£n lÃ½ Ä‘Æ°á»ng dáº«n (input/output/temp directories)
- `AppConfig`: Tá»•ng há»£p táº¥t cáº£ configs

**Why Pydantic?**
- Type validation tá»± Ä‘á»™ng
- Config serialization (JSON)
- Clear error messages

---

### 2. **transcriber.py** - Speech Recognition

**Purpose:** Chuyá»ƒn audio thÃ nh text vá»›i timestamp chÃ­nh xÃ¡c

**Key Classes:**
- `TranscriptSegment`: Data model cho má»™t Ä‘oáº¡n transcript
- `AudioTranscriber`: Main transcription engine

**Workflow:**
```
Audio File â†’ Whisper Model â†’ Raw Segments â†’ Stable-TS Alignment
â†’ Word-level Timestamps â†’ Sentence Splitting â†’ Merged Segments
```

**Key Features:**
- Sá»­ dá»¥ng `stable-whisper` cho timestamp chÃ­nh xÃ¡c hÆ¡n vanilla Whisper
- VAD (Voice Activity Detection) Ä‘á»ƒ filter noise
- Smart sentence merging dá»±a trÃªn punctuation vÃ  duration
- Export ra nhiá»u format (TXT, JSON)

**Why Stable-Whisper?**
- Vanilla Whisper cÃ³ timestamp khÃ´ng chÃ­nh xÃ¡c (Â± 0.5s)
- Stable-TS refine timestamps báº±ng mel-spectrogram alignment
- Quan trá»ng cho viá»‡c cáº¯t audio chÃ­nh xÃ¡c

---

### 3. **segmenter.py** - Audio Segmentation

**Purpose:** Cáº¯t audio file theo timestamps

**Key Classes:**
- `AudioSegmenter`: Handles audio loading, cutting, exporting

**Workflow:**
```
Audio File â†’ Load & Normalize â†’ Segment According to Timestamps
â†’ Add Padding â†’ Export Individual Files â†’ Create Manifest
```

**Key Features:**
- Support nhiá»u audio formats (WAV, MP3, FLAC, etc.)
- Automatic resampling (16kHz for Whisper compatibility)
- Silence padding Ä‘á»ƒ trÃ¡nh cáº¯t máº¥t Ã¢m thanh
- Batch export vá»›i progress tracking
- Manifest generation (JSON metadata)

**Why PyDub?**
- High-level API dá»… dÃ¹ng
- Support nhiá»u formats thÃ´ng qua ffmpeg
- Fast vÃ  memory-efficient cho audio manipulation

---

### 4. **processor.py** - Main Orchestrator

**Purpose:** Äiá»u phá»‘i toÃ n bá»™ workflow

**Key Classes:**
- `AudioProcessor`: Central coordinator

**Workflow:**
```
Input â†’ Transcribe â†’ Segment Text â†’ Cut Audio â†’ Export â†’ Metadata
```

**Key Methods:**
- `process_single_file()`: Xá»­ lÃ½ 1 file
- `process_batch()`: Xá»­ lÃ½ nhiá»u files
- `get_processing_stats()`: TÃ­nh thá»‘ng kÃª

**Responsibilities:**
1. Initialize sub-components (transcriber, segmenter)
2. Manage directories
3. Error handling vÃ  retry logic
4. Progress tracking vÃ  logging
5. Metadata generation

---

### 5. **main.py** - CLI Entry Point

**Purpose:** Command-line interface

**Features:**
- Argument parsing vá»›i argparse
- Logging setup
- Config building tá»« CLI args
- Single/batch mode switching
- Statistics viewing

**Usage Patterns:**
```bash
# Single file
python main.py --input audio.wav --output ./out

# Batch
python main.py --batch --input-dir ./audios

# Custom model
python main.py --input audio.wav --model large --device cuda
```

---

### 6. **worker.py** - Distributed Processing

**Purpose:** Multi-machine processing

**Architecture:**
```
         Shared Storage (NFS/Network Drive)
                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚           â”‚           â”‚
    Worker 1    Worker 2    Worker 3
    (CPU)       (GPU)       (CPU)
```

**Key Features:**
- File-based locking mechanism
- `.processing` vÃ  `.done` markers
- Race condition handling
- Auto-discovery cá»§a pending files
- Worker identification

**Locking Mechanism:**
```
audio.wav              # Original file
audio.wav.processing  # Lock marker (JSON with worker_id)
audio.wav.done        # Completion marker (JSON with metadata)
```

**Why This Approach?**
- Simple, no need for queue server (Redis/RabbitMQ)
- Works vá»›i any shared storage
- Easy monitoring (just check markers)
- Fault tolerant (can manually reset by deleting markers)

---

## ğŸ“Š Data Flow

### Single File Processing

```
1. INPUT
   â””â”€ audio.wav (mono/stereo, any sample rate, any format)

2. TRANSCRIPTION
   â”œâ”€ Load audio
   â”œâ”€ Resample to 16kHz mono (if needed)
   â”œâ”€ Whisper â†’ segments with rough timestamps
   â”œâ”€ Stable-TS â†’ refined word-level timestamps
   â””â”€ Sentence merging â†’ final segments

3. SEGMENTATION
   â”œâ”€ Load original audio
   â”œâ”€ For each segment:
   â”‚  â”œâ”€ Extract [start:end] with padding
   â”‚  â””â”€ Export to segment_XXXX.wav
   â””â”€ Create manifest.json

4. OUTPUT
   â”œâ”€ segment_0001.wav + segment_0001.txt
   â”œâ”€ segment_0002.wav + segment_0002.txt
   â”œâ”€ ...
   â”œâ”€ full_transcript.txt
   â”œâ”€ full_transcript.json
   â”œâ”€ manifest.json
   â””â”€ metadata.json
```

---

## ğŸ¯ Design Decisions

### Why Whisper?

**Pros:**
- State-of-the-art accuracy (English ASR)
- Open source, runs locally
- Multiple model sizes (tiny â†’ large)
- Multilingual support (68 languages)

**Cons:**
- Slow trÃªn CPU (especially large models)
- High memory usage
- Timestamp khÃ´ng perfect (needs stable-ts)

**Alternatives considered:**
- Google Speech-to-Text: Paid, needs internet
- Vosk: Faster but less accurate
- Wav2Vec2: Good but needs fine-tuning

### Why Stable-Whisper?

- Whisper's timestamps can be off by 0.5-1s
- Stable-TS adds mel-spectrogram alignment
- Critical for precise audio cutting
- Minimal overhead (~10% slower)

### Why PyDub for Audio?

**Alternatives:**
- librosa: More scientific, slower for simple ops
- scipy.io.wavfile: Low-level, less formats
- soundfile: Great but PyDub is higher-level

PyDub chosen for:
- Simple API
- Format flexibility (via ffmpeg)
- Good documentation

### Why File-Based Locking for Workers?

**Alternatives:**
- Celery + Redis: Complex setup, overkill
- Task queue systems: Need server infrastructure
- Database locking: Need DB setup

File-based locking chosen for:
- Zero infrastructure needed
- Works with any shared storage
- Easy to debug
- Simple to implement

---

## ğŸ”„ Processing Pipeline Details

### Transcription Pipeline

```python
# 1. Load model (one-time)
model = stable_whisper.load_model("base")

# 2. Transcribe
result = model.transcribe(
    audio_path,
    language="en",
    vad=True,          # Filter silence
    mel_first=True,    # Better alignment
    word_timestamps=True
)

# 3. Process segments
for segment in result.segments:
    # Each segment has:
    # - start, end (timestamps)
    # - text (transcribed text)
    # - words (word-level timestamps)
    
# 4. Merge short segments
# Logic: 
# - If segment < min_duration, merge with next
# - If text doesn't end with punctuation, merge
# - If merged > max_duration, don't merge
```

### Segmentation Pipeline

```python
# 1. Load audio
audio = AudioSegment.from_file(path)

# 2. Normalize
audio = audio.set_frame_rate(16000)  # Resample
audio = audio.set_channels(1)         # Mono

# 3. For each transcript segment
for seg in segments:
    start_ms = seg.start * 1000
    end_ms = seg.end * 1000
    
    # Add padding
    start_ms = max(0, start_ms - padding)
    end_ms = min(len(audio), end_ms + padding)
    
    # Cut audio
    chunk = audio[start_ms:end_ms]
    
    # Export
    chunk.export(f"segment_{seg.id:04d}.wav")
```

---

## ğŸ’¾ Output Formats

### 1. Individual Files

```
output/
â”œâ”€â”€ segment_0001.wav      # Audio segment 1
â”œâ”€â”€ segment_0001.txt      # "This is the first sentence."
â”œâ”€â”€ segment_0002.wav
â”œâ”€â”€ segment_0002.txt
â””â”€â”€ ...
```

**Use Case:** TTS training, ASR dataset creation

### 2. Full Transcript

```
[0.00 - 3.45] This is the first sentence.
[3.45 - 7.89] This is the second sentence.
[7.89 - 12.30] And this is the third one.
```

**Use Case:** Subtitles, reference transcription

### 3. Manifest JSON

```json
{
  "total_segments": 100,
  "total_duration": 450.5,
  "segments": [
    {
      "id": 0,
      "audio_file": "segment_0001.wav",
      "text_file": "segment_0001.txt",
      "text": "This is the first sentence.",
      "start": 0.0,
      "end": 3.45,
      "duration": 3.45
    },
    ...
  ]
}
```

**Use Case:** Training data loader, dataset analysis

---

## ğŸš€ Performance Considerations

### CPU vs GPU

| Model  | CPU (i7)  | GPU (RTX 3060) | Speedup |
|--------|-----------|----------------|---------|
| tiny   | 0.1x RT   | 0.02x RT       | 5x      |
| base   | 0.3x RT   | 0.05x RT       | 6x      |
| small  | 0.6x RT   | 0.08x RT       | 7.5x    |
| medium | 1.5x RT   | 0.15x RT       | 10x     |
| large  | 3.0x RT   | 0.25x RT       | 12x     |

*RT = Real Time (1x RT = 1 phÃºt audio = 1 phÃºt xá»­ lÃ½)*

### Bottlenecks

1. **Transcription** (70-80% of time)
   - Solution: Use GPU, smaller model
   
2. **Audio I/O** (10-15%)
   - Solution: Use SSD, batch processing
   
3. **Segmentation** (5-10%)
   - Negligible, well-optimized

### Optimization Tips

```python
# 1. Use GPU
config = WhisperConfig(device="cuda")

# 2. Use appropriate model
# base for most use cases
# large only if accuracy critical

# 3. Batch processing
processor.process_batch()  # Better than loop

# 4. Multi-machine
# Scale horizontally with workers
```

---

## ğŸ§ª Testing Strategy

### Unit Tests

- `test_config.py`: Config validation
- `test_transcriber.py`: Transcription accuracy
- `test_segmenter.py`: Audio cutting precision
- `test_processor.py`: End-to-end workflow

### Integration Tests

- Full pipeline with sample audio
- Edge cases (silence, noise, music)
- Format compatibility (MP3, FLAC, etc.)

### Performance Tests

- Benchmark different models
- Memory profiling
- Speed comparisons

---

## ğŸ“ˆ Future Enhancements

### Planned Features

1. **Speaker Diarization**
   - PhÃ¢n biá»‡t nhiá»u ngÆ°á»i nÃ³i
   - Useful cho interviews, meetings
   
2. **Quality Filtering**
   - Tá»± Ä‘á»™ng loáº¡i bá» segments cÃ³ quality tháº¥p
   - WER (Word Error Rate) estimation

3. **Web UI**
   - Drag-and-drop interface
   - Real-time progress
   - Results visualization

4. **Cloud Storage Integration**
   - S3, Google Cloud Storage
   - Azure Blob Storage

5. **API Server**
   - REST API endpoint
   - Job queue management
   - Authentication

---

## ğŸ› ï¸ Maintenance

### Adding New Audio Format

1. Ensure ffmpeg supports it
2. Test with PyDub: `AudioSegment.from_file(path, format="xxx")`
3. Add to supported formats list

### Updating Whisper Model

```python
# In transcriber.py
model = stable_whisper.load_model("large-v3")  # Latest version
```

### Customizing Segment Merging Logic

```python
# In transcriber.py â†’ transcribe_to_sentences()
should_merge = (
    current_segment.duration < min_duration or
    not current_segment.text.rstrip().endswith(('.', '!', '?'))
    # Add your custom logic here
) and potential_duration <= max_duration
```

---

**For more details, see the code comments in each module.**
